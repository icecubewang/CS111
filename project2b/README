NAME: Feilan Wang
EMAIL: wangfeilan@hotmail.com
ID: 104796844

==================================================================================

Descriptions of each of the included files in (.tar.gz):

1. SortedList.h - a header file containing interfaces for linked list operations

2. SortedList.c - the source for a C source module, it implements insert, delete, lookup, and length methods for a sorted doubly linked list

3. lab2_list.c - the source for a C program, it implements the specified command line options (--threads, --iterations, --yield, --sync, --lists)

4. Makefile - to build the deliverable programs, output, graphs, and tarball
		tests: run all specified test cases to generate CSV results
		profile: run tests with profiling tools to generate an execution profiling report
		graphs: use gnuplot to generate the required graphs
		dist: create the deliverable tarball
		clean: delete all programs and output generated by the Makefile 

5. lab2b_list.csv - containing the results for all of test runs

6. profile.out - execution profiling report showing where time was spent in the un-partitioned spin-lock implementation

7. graphs
		lab2b_1.png: throughput vs. number of threads for mutex and spin-lock synchronized list operations
		lab2b_2.png: mean time per mutex wait and mean time per operation for mutex-synchronized list operations
		lab2b_3.png: successful iterations vs. threads for each synchronization method
		lab2b_4.png: throughput vs. number of threads for mutex synchronized partitioned lists
		lab2b_5.png: throughput vs. number of threads for spin-lock-synchronized partitioned lists.

8. gnuplotGenerate.sh - executable file to generate the data for lab2b_list.csv

9. README - this file

==================================================================================

Brief (a few sentences per question) answers to each of the questions:
----------------------------------------------------------------------------------

QUESTION 2.3.1 - Cycles in the basic list implementation:
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests? 
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

ANSWER 
(1) I believe most of the cycles are spent in list operations (mostly insertion) in the 1 and 2-thread list tests. 
(2) It is unlikely to be due to context switch because there are only 1 or 2 thread. And also there's no thread contending for lock.
(3) Most of the time/cycles are being spent in spinning, because there are a large number of threads waiting for lock.
(4) Most of the time/cycles are being spent in context switch. The threads waiting for lock will be put to sleep, so that the CPU will do context switch to let other threads to run. And this process occurs frequently. 

----------------------------------------------------------------------------------

QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
Why does this operation become so expensive with large numbers of threads?

ANSWER
(1) By looking at profile.out, most of the time is spent on SortedList_insert and SortedList_lookup. Most of the times goes to the following two lines of execution:
   221    221   86: 			while (__sync_lock_test_and_set(&lock_testAndSet[whichlist], 1));
   170    170  146: 			while (__sync_lock_test_and_set(&lock_testAndSet[whichlist], 1));
(2) The operation becomes expansive with large numbers of threads because as more threads are contending for the lock, more of them will be put into spinning, which cost CPU time. 

----------------------------------------------------------------------------------

QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

ANSWER
(1) The average lock-wait time rise dramatically because as the number of threads increases, more threads are contending for the lock, so every threads have to wait for a longer time. This time is based on CPU time so it rises dramatically.
(2) The completion time rises because there's more contention for lock and context-switch as the number of threads goes up. However, it does not rise as dramatically because its time is based on wall time. 
(3) Because the wait time per operation is the cumulation of all threads, while the completion time per operation is the wall time that entire program takes. When there are multiple threads waiting at the same time, the time added to wait time per operation is the time * number of threads waiting, while the time added to completion time is just the time. 

---------------------------------------------------------------------------------

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

ANSWER
(1) The throughput increases with the number of lists. This is because with more sublists, each sublists have its own lock, the contention for the lock drops. So threads spend less time waiting for lock. Also, the length of the list decreases, so the cost of traversal for instert and look up also decreases. 
(2) No. Throughput increases at a decreasing rate as the number of lists increases. This is because as the number of sublists increases, the time spent on contending for the lock is less significant compared to the time spent on list operation. Eventually, the cost of lock contention will be almost zero. At this point onwards, the thorughput will be rather steady. 
(3) No. Both the number of threads and the number of sublists will determine the chance of lock contention. A single list with fewer (1/N) threads will result in longer list. So the cost of traversal becomes larger than many lists. 

---------------------------------------------------------------------------------

